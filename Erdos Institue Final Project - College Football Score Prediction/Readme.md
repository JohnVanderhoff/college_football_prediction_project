# Final Project Folder: College Football Prediction
Final submission folder for the 2024 Erdos Institute Data Science project: College Football outcome prediction model

Team members: Nate Clause and John Vanderhoff

## Project Description
College football is one of the most popular sports in the US, with tens of millions of viewers tuning in to watch every week and billions of dollars generated by the programs. Furthermore, college football betting is experiencing a massive boom as many states have legalized the practice over the past decade. These factors and more generate a desire to generate predictions of the outcomes of college football games ahead of time. Our project develops models to make predictions on some outcome types for college football games, including a variety of game stats, and most notably the final score itself.

Stakeholders include TV networks, who have motivation to broadcast the most exciting games, the sports betting industry who want to determine the betting lines which will maximize their profits, and the individual teams themselves who want to utilize analytics to optimize their practice and approach to the game.

## Data and Approach
We gathered data on all FBS games from 2014-2023 through the api on <a href="(https://collegefootballdata.com/)">College football data</a>. We then cleaned the data and restricted to 29 statistical categories per team per game. There were then three different model types we developed:
- Model 1: this type of model takes in recent historical game statistics for the two teams playing in an upcoming game and tries to predict their game statistics for the upcoming game.
- Model 2: this type of model takes in the actual statistics which occurred in a game and tries to predict the final score of that game.
- Model 3: this type of model functions as a concatenation of Model types 1 and 2; taking in recent historical game statistics for the two teams playing in an upcoming game and trying to predict the final score for the upcoming game.

We made an initial approach to all the model types with classical regression methods, including linear regression, ridge regression, lasso regression, and elastic net regression, all with cross-validation using the built-in functions from Scikit-Learn. The file "Prediction with Regression Models.ipynb" contains the Jupyter notebook with all corresponding code and results.

After this, we developed deep learning models for model types 1 and 3. We utilized the long-short-term-memory (LSTM) architecture for these models, and LSTM is known to work well with time-series data, such as the recent historical statistics which are the input to these models. Both models consisted of the input layer, 3 LSTM layers, and a final dense layer for the output. The file "Prediction with Deep Learning Models.ipynb" contains the Jupyter notebook with all corresponding code and results.

## Results

Model 1 is the most difficult type of model for us to analyze, as many game stats have wildly different magnitudes. After exploration, we saw that the vast majority of the game stats followed a normal distribution. As a result, we utilized Scikit-Learn's standard scaler to scale down all stats to have mean 0 and variance 1. We then analyzed the performance via mean squared error (MSE). The classical regression models performed similarly, with the Elastic Net with cross-validation models performing the best, achieving a MSE of approximately 0.84 between the holdout test data and the predictions on the test data. This is close to one full standard deviation from the mean, so the performance is not particularly strong. As a result, we developed the LSTM model as described previously. The MSE between the test data and predictions on input test data was 0.57. This is notably improved, but there may still be room for improvement as we discuss below.

Model 2 we only utilized classical regression approaches. This is because we got very good results, namely the Lasso with cross validation model saw a mean absolute error of ~0.012 points! This means that on average an individual score was less than 2 hundredths of a point off of the actual score. Furthermore, this model correctly predicted the winner 100% of the time. This demonstrates a very strong model and we saw no need to attempt a more sophisticated modelling approach for this model type. As a note, models types 1 and 3 both incorporate data directly tied to scoring such as number of rushing TDs and number of passing TDs. Given that this information would be "cheating" for Model 2, we removed any features such as this that directly correlated with score in the most literal sense, and it still achieved the performance noted above!

Model 3 started with classical regression approaches, with the best performing model we developed being the Lasso model with cross validation, having a mean absolute error on the score of ~9.8 points. This is rather high, with the model correctly predicting the winner only approximately 71.3% of the time. We noted that the distribution of the predicted scores had the same overall shape as the distribution of the actual scores, but was more compressed towards the average. This made us hypothesize that the linear relationships predicted via the model were insufficient for dealing with games at more extreme score ranges. As a result, we implemented the deep learning model described earlier. This model achieved much better results, with a mean absolute error on the score of ~3.45 points. This model correctly predicted the winner approximately 90.5% of the time. According to <a href="https://philsteele.com/how-often-to-underdogs-win-outright/">Phil Steele's underdog analysis</a>, the favorites based on the Las Vegas sportsbooks only win around 76.7% of the time, so our model is outperforming this significantly. See the figure below:

<figure>
  <img src="![image](https://github.com/JohnVanderhoff/college_football_prediction_project/assets/25011329/0e48ed62-6141-4cd2-afc2-3c0bcdafe50d)
" alt="True scores and predicted scores" style="width:100%">
  <figcaption>A plot of the scores which occurred in a game and the predicted scores given by our LSTM model for model type 3.</figcaption>
</figure>

## Future Directions
There are more features which we did not add in to the model which could improve performance. Some features that we think may improve model performance but which we did not include due to time limitations and/or difficulty finding data sources include average field position and weather. The most notable feature(s) to add in to the model relate to overall quality of a team in a universal sense. Usually, the games where our models 1 and 3 struggled the most involved a school who consistently put up very good stats against mediocre competition losing to a team who put up average stats against elite competition. Having a more universal sense of team quality through could be managed through something like recruiting or an elo system, and would likely help deal with these types of games our current models 1 and 3 struggle with.
