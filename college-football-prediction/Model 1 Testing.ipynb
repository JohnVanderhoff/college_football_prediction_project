{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1\n",
    "In this model in our two-model layer, we start with input the recent historical stats of the teams in a game and predict the stats for the team in their pending matchup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.linear_model import Ridge\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>school_id</th>\n",
       "      <th>points</th>\n",
       "      <th>fumblesRecovered</th>\n",
       "      <th>rushingTDs</th>\n",
       "      <th>passingTDs</th>\n",
       "      <th>kickReturnYards</th>\n",
       "      <th>kickReturnTDs</th>\n",
       "      <th>kickReturns</th>\n",
       "      <th>kickingPoints</th>\n",
       "      <th>...</th>\n",
       "      <th>opposing_rushingTDs</th>\n",
       "      <th>opposing_points</th>\n",
       "      <th>side</th>\n",
       "      <th>thirdDownConverts</th>\n",
       "      <th>thirdDownAttempts</th>\n",
       "      <th>fourthDownConverts</th>\n",
       "      <th>fourthDownAttempts</th>\n",
       "      <th>completions</th>\n",
       "      <th>passAttempts</th>\n",
       "      <th>passCompletionPercentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Navy</td>\n",
       "      <td>2426</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34</td>\n",
       "      <td>A</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UAB</td>\n",
       "      <td>5</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UMass</td>\n",
       "      <td>113</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30</td>\n",
       "      <td>A</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.409091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UCF</td>\n",
       "      <td>2116</td>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26</td>\n",
       "      <td>A</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Houston</td>\n",
       "      <td>248</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>27</td>\n",
       "      <td>A</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    school  school_id  points  fumblesRecovered  rushingTDs  passingTDs  \\\n",
       "0     Navy       2426      17               0.0         2.0         0.0   \n",
       "1      UAB          5      48               1.0         4.0         2.0   \n",
       "2    UMass        113       7               0.0         0.0         1.0   \n",
       "3      UCF       2116      24               1.0         2.0         1.0   \n",
       "4  Houston        248       7               1.0         1.0         0.0   \n",
       "\n",
       "   kickReturnYards  kickReturnTDs  kickReturns  kickingPoints  ...  \\\n",
       "0             64.0            0.0          4.0            5.0  ...   \n",
       "1             64.0            0.0          2.0           12.0  ...   \n",
       "2             95.0            0.0          4.0            1.0  ...   \n",
       "3            142.0            0.0          5.0            6.0  ...   \n",
       "4             89.0            0.0          6.0            1.0  ...   \n",
       "\n",
       "   opposing_rushingTDs  opposing_points  side  thirdDownConverts  \\\n",
       "0                  1.0               34     A                4.0   \n",
       "1                  1.0               10     A                9.0   \n",
       "2                  2.0               30     A                3.0   \n",
       "3                  1.0               26     A                5.0   \n",
       "4                  3.0               27     A                4.0   \n",
       "\n",
       "   thirdDownAttempts  fourthDownConverts  fourthDownAttempts  completions  \\\n",
       "0               12.0                 1.0                 1.0          2.0   \n",
       "1               14.0                 0.0                 0.0         13.0   \n",
       "2               11.0                 1.0                 1.0          9.0   \n",
       "3               13.0                 2.0                 2.0         12.0   \n",
       "4               16.0                 4.0                 4.0         25.0   \n",
       "\n",
       "  passAttempts  passCompletionPercentage  \n",
       "0          4.0                  0.500000  \n",
       "1         20.0                  0.650000  \n",
       "2         22.0                  0.409091  \n",
       "3         22.0                  0.545455  \n",
       "4         50.0                  0.500000  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "games_df = pd.read_csv(\"clean_games_with_opp.csv\", index_col=0)\n",
    "games_df.reset_index(drop=True, inplace=True)\n",
    "games_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all columns with more than 25 nulls, then dropping the individual rows that have nulls\n",
    "x = games_df.isna().sum().to_dict()\n",
    "column_drop = []\n",
    "\n",
    "for k, v in x.items():\n",
    "    if v > 25 :\n",
    "        column_drop.append(k)\n",
    "\n",
    "# Additionally, these columns are weird formats or already covered by other variables (ie the possession stuff)\n",
    "extend_list = ['completionAttempts','totalPenaltiesYards','possessionTime','possession_minutes','possession_seconds','year','week','school','opposing_points']\n",
    "\n",
    "#column_drop.extend(extend_list)\n",
    "column_drop = column_drop + extend_list\n",
    "\n",
    "games_df.drop(columns=column_drop, inplace = True)\n",
    "games_df.dropna(axis = 0, inplace = True)\n",
    "\n",
    "games_df = games_df.select_dtypes(exclude=[\"object\"])\n",
    "\n",
    "games_df = games_df.sort_values(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['school_id', 'points', 'fumblesRecovered', 'rushingTDs', 'passingTDs',\n",
       "       'firstDowns', 'thirdDownEff', 'totalYards', 'netPassingYards',\n",
       "       'yardsPerPass', 'rushingYards', 'rushingAttempts',\n",
       "       'yardsPerRushAttempt', 'turnovers', 'fumblesLost', 'interceptions',\n",
       "       'poss_total_sec', 'id', 'opposing_netPassingYards',\n",
       "       'opposing_yardsPerPass', 'opposing_rushingYards',\n",
       "       'opposing_yardsPerRushAttempt', 'opposing_passingTDs',\n",
       "       'opposing_rushingTDs', 'thirdDownConverts', 'thirdDownAttempts',\n",
       "       'fourthDownConverts', 'fourthDownAttempts', 'completions',\n",
       "       'passAttempts', 'passCompletionPercentage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "games_df.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initially set the historical memory equal to 12 games. This is roughly the past seasons-worth of games for an average team. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2, 2050, 5, 6, 2567, 8, 2569, 9, 2571, 2572, 12, 13, 16, 2065, 2579, 21, 2582, 23, 24, 25, 26, 3101, 30, 2083, 36, 2084, 38, 2598, 41, 43, 47, 48, 2097, 50, 52, 55, 57, 58, 2619, 59, 61, 2110, 2617, 2623, 62, 66, 2115, 2116, 68, 2628, 2117, 70, 2633, 2630, 2635, 2636, 77, 2638, 79, 2127, 2641, 2634, 2643, 2132, 84, 2640, 87, 2649, 2653, 93, 2655, 96, 97, 98, 99, 2142, 103, 107, 113, 2674, 2678, 119, 120, 2169, 2681, 127, 130, 2692, 2181, 135, 2184, 142, 145, 2193, 147, 2197, 150, 151, 152, 153, 154, 2711, 2199, 2717, 158, 2710, 160, 155, 2210, 164, 166, 167, 2729, 2226, 2229, 2230, 183, 2747, 189, 2751, 193, 194, 2755, 2241, 197, 2754, 2247, 195, 201, 202, 204, 2771, 213, 2261, 218, 221, 222, 227, 228, 2277, 231, 233, 235, 236, 238, 239, 149, 2287, 242, 2803, 245, 2294, 248, 249, 2296, 251, 252, 253, 254, 256, 2305, 258, 259, 257, 2309, 2306, 264, 265, 2320, 275, 276, 277, 278, 2837, 2329, 282, 2335, 290, 2341, 295, 2348, 301, 302, 304, 2198, 309, 311, 322, 324, 326, 328, 2377, 331, 333, 2382, 338, 2390, 344, 2393, 349, 2400, 356, 2916, 2405, 2415, 2426, 2428, 2429, 2433, 2439, 2440, 2447, 2448, 2449, 2450, 399, 2453, 2458, 2459, 2460, 2464, 2466, 2627, 2483, 2502, 2504, 2506, 2509, 2000, 2005, 2006, 2010, 2011, 2523, 2016, 2529, 2534, 2535, 2026, 2029, 2542, 2032, 2545, 2546, 2046}\n",
      "245\n"
     ]
    }
   ],
   "source": [
    "teams = set(games_df[\"school_id\"])\n",
    "print(teams)\n",
    "print(len(teams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for team in teams:\n",
    "    team_df = games_df[games_df[\"school_id\"] == team]\n",
    "    team_df = team_df.sort_values(\"id\")\n",
    "    team_df.drop(columns=[\"id\", \"school_id\"], inplace=True)\n",
    "    team_array = np.array(team_df)\n",
    "    \n",
    "    for i in range(memory, len(team_array)):\n",
    "        X.append(np.hstack(team_array[i-memory:i]))\n",
    "        y.append(team_array[i])\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the data into the training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the train and test data, we will scale the input to the model. Since the output is stats each working on the same scales as the inputs, we scale the output as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "scaler2.fit(y_train)\n",
    "y_train_scaled = scaler2.transform(y_train)\n",
    "y_test_scaled = scaler2.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing models\n",
    "We first try some linear models for predicting the stats in the next game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Ridge(alpha=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(alpha=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Ridge(alpha=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, ElasticNet, Lasso, LinearRegression\n",
    "\n",
    "reg25 = Ridge(alpha=0.25)\n",
    "reg25.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "reg50 = Ridge(alpha=0.5)\n",
    "reg50.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "reg100 = Ridge(alpha=1)\n",
    "reg100.fit(X_train_scaled, y_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now make predictions with the ridge models on the test set and compute the mean squared error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: MSE are: \n",
      " 0.25:0.9313190372921086 \n",
      " 0.50:0.9313196486549717 \n",
      " 1.00:0.9312959331776132\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pred_scores25 = reg25.predict(X_test_scaled)\n",
    "pred_scores50 = reg50.predict(X_test_scaled)\n",
    "pred_scores100 = reg100.predict(X_test_scaled)\n",
    "\n",
    "print(f\"alpha: MSE are: \\n 0.25:{mean_squared_error(pred_scores25, y_test_scaled)} \\n 0.50:{mean_squared_error(pred_scores50, y_test_scaled)} \\n 1.00:{mean_squared_error(pred_scores100, y_test_scaled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE are \n",
      " Lasso:0.9949641965238859\n",
      " ElasticNet:0.9949641965238859\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(alpha=1)\n",
    "lasso.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "ela_net = ElasticNet(alpha=1)\n",
    "ela_net.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "print(f\"MSE are \\n Lasso:{mean_squared_error(lasso.predict(X_test_scaled), y_test_scaled)}\\n ElasticNet:{mean_squared_error(ela_net.predict(X_test_scaled), y_test_scaled)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous modelling attempts, we predicted the stats for a given team based only on that team's history. Now we will try to look at predicting stats for a given game based on the combined histories of the two teams playing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_dict = {}\n",
    "for team in teams:\n",
    "    temp_df = games_df[games_df[\"school_id\"] == team].sort_values(\"id\")\n",
    "    #temp_df.reset_index(inplace=True)\n",
    "    teams_dict[team] = temp_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on team 2\n",
      "on team 2050\n",
      "on team 5\n",
      "on team 6\n",
      "on team 2567\n",
      "on team 8\n",
      "on team 2569\n",
      "on team 9\n",
      "on team 2571\n",
      "on team 2572\n",
      "on team 12\n",
      "on team 13\n",
      "on team 16\n",
      "on team 2065\n",
      "on team 2579\n",
      "on team 21\n",
      "on team 2582\n",
      "on team 23\n",
      "on team 24\n",
      "on team 25\n",
      "on team 26\n",
      "on team 3101\n",
      "on team 30\n",
      "on team 2083\n",
      "on team 36\n",
      "on team 2084\n",
      "on team 38\n",
      "on team 2598\n",
      "on team 41\n",
      "on team 43\n",
      "on team 47\n",
      "on team 48\n",
      "on team 2097\n",
      "on team 50\n",
      "on team 52\n",
      "on team 55\n",
      "on team 57\n",
      "on team 58\n",
      "on team 2619\n",
      "on team 59\n",
      "on team 61\n",
      "on team 2110\n",
      "on team 2617\n",
      "on team 2623\n",
      "on team 62\n",
      "on team 66\n",
      "on team 2115\n",
      "on team 2116\n",
      "on team 68\n",
      "on team 2628\n",
      "on team 2117\n",
      "on team 70\n",
      "on team 2633\n",
      "on team 2630\n",
      "on team 2635\n",
      "on team 2636\n",
      "on team 77\n",
      "on team 2638\n",
      "on team 79\n",
      "on team 2127\n",
      "on team 2641\n",
      "on team 2634\n",
      "on team 2643\n",
      "on team 2132\n",
      "on team 84\n",
      "on team 2640\n",
      "on team 87\n",
      "on team 2649\n",
      "on team 2653\n",
      "on team 93\n",
      "on team 2655\n",
      "on team 96\n",
      "on team 97\n",
      "on team 98\n",
      "on team 99\n",
      "on team 2142\n",
      "on team 103\n",
      "on team 107\n",
      "on team 113\n",
      "on team 2674\n",
      "on team 2678\n",
      "on team 119\n",
      "on team 120\n",
      "on team 2169\n",
      "on team 2681\n",
      "on team 127\n",
      "on team 130\n",
      "on team 2692\n",
      "on team 2181\n",
      "on team 135\n",
      "on team 2184\n",
      "on team 142\n",
      "on team 145\n",
      "on team 2193\n",
      "on team 147\n",
      "on team 2197\n",
      "on team 150\n",
      "on team 151\n",
      "on team 152\n",
      "on team 153\n",
      "on team 154\n",
      "on team 2711\n",
      "on team 2199\n",
      "on team 2717\n",
      "on team 158\n",
      "on team 2710\n",
      "on team 160\n",
      "on team 155\n",
      "on team 2210\n",
      "on team 164\n",
      "on team 166\n",
      "on team 167\n",
      "on team 2729\n",
      "on team 2226\n",
      "on team 2229\n",
      "on team 2230\n",
      "on team 183\n",
      "on team 2747\n",
      "on team 189\n",
      "on team 2751\n",
      "on team 193\n",
      "on team 194\n",
      "on team 2755\n",
      "on team 2241\n",
      "on team 197\n",
      "on team 2754\n",
      "on team 2247\n",
      "on team 195\n",
      "on team 201\n",
      "on team 202\n",
      "on team 204\n",
      "on team 2771\n",
      "on team 213\n",
      "on team 2261\n",
      "on team 218\n",
      "on team 221\n",
      "on team 222\n",
      "on team 227\n",
      "on team 228\n",
      "on team 2277\n",
      "on team 231\n",
      "on team 233\n",
      "on team 235\n",
      "on team 236\n",
      "on team 238\n",
      "on team 239\n",
      "on team 149\n",
      "on team 2287\n",
      "on team 242\n",
      "on team 2803\n",
      "on team 245\n",
      "on team 2294\n",
      "on team 248\n",
      "on team 249\n",
      "on team 2296\n",
      "on team 251\n",
      "on team 252\n",
      "on team 253\n",
      "on team 254\n",
      "on team 256\n",
      "on team 2305\n",
      "on team 258\n",
      "on team 259\n",
      "on team 257\n",
      "on team 2309\n",
      "on team 2306\n",
      "on team 264\n",
      "on team 265\n",
      "on team 2320\n",
      "on team 275\n",
      "on team 276\n",
      "on team 277\n",
      "on team 278\n",
      "on team 2837\n",
      "on team 2329\n",
      "on team 282\n",
      "on team 2335\n",
      "on team 290\n",
      "on team 2341\n",
      "on team 295\n",
      "on team 2348\n",
      "on team 301\n",
      "on team 302\n",
      "on team 304\n",
      "on team 2198\n",
      "on team 309\n",
      "on team 311\n",
      "on team 322\n",
      "on team 324\n",
      "on team 326\n",
      "on team 328\n",
      "on team 2377\n",
      "on team 331\n",
      "on team 333\n",
      "on team 2382\n",
      "on team 338\n",
      "on team 2390\n",
      "on team 344\n",
      "on team 2393\n",
      "on team 349\n",
      "on team 2400\n",
      "on team 356\n",
      "on team 2916\n",
      "on team 2405\n",
      "on team 2415\n",
      "on team 2426\n",
      "on team 2428\n",
      "on team 2429\n",
      "on team 2433\n",
      "on team 2439\n",
      "on team 2440\n",
      "on team 2447\n",
      "on team 2448\n",
      "on team 2449\n",
      "on team 2450\n",
      "on team 399\n",
      "on team 2453\n",
      "on team 2458\n",
      "on team 2459\n",
      "on team 2460\n",
      "on team 2464\n",
      "on team 2466\n",
      "on team 2627\n",
      "on team 2483\n",
      "on team 2502\n",
      "on team 2504\n",
      "on team 2506\n",
      "on team 2509\n",
      "on team 2000\n",
      "on team 2005\n",
      "on team 2006\n",
      "on team 2010\n",
      "on team 2011\n",
      "on team 2523\n",
      "on team 2016\n",
      "on team 2529\n",
      "on team 2534\n",
      "on team 2535\n",
      "on team 2026\n",
      "on team 2029\n",
      "on team 2542\n",
      "on team 2032\n",
      "on team 2545\n",
      "on team 2546\n",
      "on team 2046\n"
     ]
    }
   ],
   "source": [
    "X2 = []\n",
    "y2 = []\n",
    "for team in teams:\n",
    "    team1_df = teams_dict[team]\n",
    "    print(f\"on team {team}\")\n",
    "    \n",
    "    # loop over all possible games for a given team with enough prior games\n",
    "    for i in range(memory, len(team1_df)):\n",
    "        # find the two teams participating in the game\n",
    "        game_id = team1_df.loc[i][\"id\"]\n",
    "        both_teams_ids = games_df[games_df[\"id\"] == game_id][\"school_id\"]\n",
    "        both_teams_ids = list(both_teams_ids)\n",
    "\n",
    "        # throw out some odd cases where  there was only data for one of the teams in a given game\n",
    "        if len(both_teams_ids) != 2:\n",
    "            continue\n",
    "        #print(f\"both teams are {both_teams_ids}\")\n",
    " \n",
    "        # extract the dataframes for the two teams, find which game it was for them\n",
    "        t1_df = teams_dict[both_teams_ids[0]]#.loc[i-memory:i+1]\n",
    "        t2_df = teams_dict[both_teams_ids[1]]#.loc[i-memory:i+1]\n",
    "\n",
    "        t1_gamen = np.where(t1_df[\"id\"] == game_id)[0][0]\n",
    "        t2_gamen = np.where(t2_df[\"id\"] == game_id)[0][0]\n",
    "\n",
    "        # check to make sure that both teams have enough games in the past\n",
    "        if t1_gamen < memory or t2_gamen < memory:\n",
    "            continue\n",
    "        \n",
    "        # extract the games in recent history, dropping columns that don't go into the data\n",
    "        t1_df = t1_df.loc[t1_gamen-memory:t1_gamen]#+1]\n",
    "        t2_df = t2_df.loc[t2_gamen-memory:t2_gamen]#+1]\n",
    "        t1_df.drop(columns=[\"id\", \"school_id\"], inplace=True)\n",
    "        t2_df.drop(columns=[\"id\", \"school_id\"], inplace=True)\n",
    "\n",
    "        t1_array = np.array(t1_df)\n",
    "        t2_array = np.array(t2_df)\n",
    "\n",
    "        # break the arrays into past games and current game to go into training and test set\n",
    "        t1_past = t1_array[0:-1]\n",
    "        t1_present = t1_array[-1]\n",
    "        t2_past = t2_array[0:-1]\n",
    "        t2_present = t2_array[-1]\n",
    "\n",
    "\n",
    "        X2.append(np.hstack([t1_past, t2_past]))\n",
    "        y2.append(np.hstack([t1_present, t2_present]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_flat = []\n",
    "for i in range(len(X2)):\n",
    "    X2_flat.append(X2[i].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12742, 672) (12742, 56)\n"
     ]
    }
   ],
   "source": [
    "X2_flat = np.array(X2_flat)\n",
    "y2 = np.array(y2)\n",
    "print(X2_flat.shape, y2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we scale the data, split it into train and test sets, and then train and test various models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2_flat, y2, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X2_train)\n",
    "X2_train_scaled = scaler.transform(X2_train)\n",
    "X2_test_scaled = scaler.transform(X2_test)\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "scaler2.fit(y2_train)\n",
    "y2_train_scaled = scaler2.transform(y2_train)\n",
    "y2_test_scaled = scaler2.transform(y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Ridge(alpha=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(alpha=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Ridge(alpha=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg25 = Ridge(alpha=0.25)\n",
    "reg25.fit(X2_train_scaled, y2_train_scaled)\n",
    "\n",
    "reg50 = Ridge(alpha=0.5)\n",
    "reg50.fit(X2_train_scaled, y2_train_scaled)\n",
    "\n",
    "reg100 = Ridge(alpha=1)\n",
    "reg100.fit(X2_train_scaled, y2_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: MSE are: \n",
      " 0.25:0.856699449691887 \n",
      " 0.50:0.8567345821663706 \n",
      " 1.00:0.8566922650408936\n"
     ]
    }
   ],
   "source": [
    "pred_scores25 = reg25.predict(X2_test_scaled)\n",
    "pred_scores50 = reg50.predict(X2_test_scaled)\n",
    "pred_scores100 = reg100.predict(X2_test_scaled)\n",
    "\n",
    "print(f\"alpha: MSE are: \\n 0.25:{mean_squared_error(pred_scores25, y2_test_scaled)} \\n 0.50:{mean_squared_error(pred_scores50, y2_test_scaled)} \\n 1.00:{mean_squared_error(pred_scores100, y2_test_scaled)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the performance is improved by considering the historical data of both teams, not just each team independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Approach with LSTM\n",
    "We now develop a model utilizing neural networks. The model will take in the recent historical data of the two teams and output the predicted stats of the next game. We utilize an LSTM model in our approach, a standard technique when dealing with time-varying inputs as we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on team 2\n",
      "on team 2050\n",
      "on team 5\n",
      "on team 6\n",
      "on team 2567\n",
      "on team 8\n",
      "on team 2569\n",
      "on team 9\n",
      "on team 2571\n",
      "on team 2572\n",
      "on team 12\n",
      "on team 13\n",
      "on team 16\n",
      "on team 2065\n",
      "on team 2579\n",
      "on team 21\n",
      "on team 2582\n",
      "on team 23\n",
      "on team 24\n",
      "on team 25\n",
      "on team 26\n",
      "on team 3101\n",
      "on team 30\n",
      "on team 2083\n",
      "on team 36\n",
      "on team 2084\n",
      "on team 38\n",
      "on team 2598\n",
      "on team 41\n",
      "on team 43\n",
      "on team 47\n",
      "on team 48\n",
      "on team 2097\n",
      "on team 50\n",
      "on team 52\n",
      "on team 55\n",
      "on team 57\n",
      "on team 58\n",
      "on team 2619\n",
      "on team 59\n",
      "on team 61\n",
      "on team 2110\n",
      "on team 2617\n",
      "on team 2623\n",
      "on team 62\n",
      "on team 66\n",
      "on team 2115\n",
      "on team 2116\n",
      "on team 68\n",
      "on team 2628\n",
      "on team 2117\n",
      "on team 70\n",
      "on team 2633\n",
      "on team 2630\n",
      "on team 2635\n",
      "on team 2636\n",
      "on team 77\n",
      "on team 2638\n",
      "on team 79\n",
      "on team 2127\n",
      "on team 2641\n",
      "on team 2634\n",
      "on team 2643\n",
      "on team 2132\n",
      "on team 84\n",
      "on team 2640\n",
      "on team 87\n",
      "on team 2649\n",
      "on team 2653\n",
      "on team 93\n",
      "on team 2655\n",
      "on team 96\n",
      "on team 97\n",
      "on team 98\n",
      "on team 99\n",
      "on team 2142\n",
      "on team 103\n",
      "on team 107\n",
      "on team 113\n",
      "on team 2674\n",
      "on team 2678\n",
      "on team 119\n",
      "on team 120\n",
      "on team 2169\n",
      "on team 2681\n",
      "on team 127\n",
      "on team 130\n",
      "on team 2692\n",
      "on team 2181\n",
      "on team 135\n",
      "on team 2184\n",
      "on team 142\n",
      "on team 145\n",
      "on team 2193\n",
      "on team 147\n",
      "on team 2197\n",
      "on team 150\n",
      "on team 151\n",
      "on team 152\n",
      "on team 153\n",
      "on team 154\n",
      "on team 2711\n",
      "on team 2199\n",
      "on team 2717\n",
      "on team 158\n",
      "on team 2710\n",
      "on team 160\n",
      "on team 155\n",
      "on team 2210\n",
      "on team 164\n",
      "on team 166\n",
      "on team 167\n",
      "on team 2729\n",
      "on team 2226\n",
      "on team 2229\n",
      "on team 2230\n",
      "on team 183\n",
      "on team 2747\n",
      "on team 189\n",
      "on team 2751\n",
      "on team 193\n",
      "on team 194\n",
      "on team 2755\n",
      "on team 2241\n",
      "on team 197\n",
      "on team 2754\n",
      "on team 2247\n",
      "on team 195\n",
      "on team 201\n",
      "on team 202\n",
      "on team 204\n",
      "on team 2771\n",
      "on team 213\n",
      "on team 2261\n",
      "on team 218\n",
      "on team 221\n",
      "on team 222\n",
      "on team 227\n",
      "on team 228\n",
      "on team 2277\n",
      "on team 231\n",
      "on team 233\n",
      "on team 235\n",
      "on team 236\n",
      "on team 238\n",
      "on team 239\n",
      "on team 149\n",
      "on team 2287\n",
      "on team 242\n",
      "on team 2803\n",
      "on team 245\n",
      "on team 2294\n",
      "on team 248\n",
      "on team 249\n",
      "on team 2296\n",
      "on team 251\n",
      "on team 252\n",
      "on team 253\n",
      "on team 254\n",
      "on team 256\n",
      "on team 2305\n",
      "on team 258\n",
      "on team 259\n",
      "on team 257\n",
      "on team 2309\n",
      "on team 2306\n",
      "on team 264\n",
      "on team 265\n",
      "on team 2320\n",
      "on team 275\n",
      "on team 276\n",
      "on team 277\n",
      "on team 278\n",
      "on team 2837\n",
      "on team 2329\n",
      "on team 282\n",
      "on team 2335\n",
      "on team 290\n",
      "on team 2341\n",
      "on team 295\n",
      "on team 2348\n",
      "on team 301\n",
      "on team 302\n",
      "on team 304\n",
      "on team 2198\n",
      "on team 309\n",
      "on team 311\n",
      "on team 322\n",
      "on team 324\n",
      "on team 326\n",
      "on team 328\n",
      "on team 2377\n",
      "on team 331\n",
      "on team 333\n",
      "on team 2382\n",
      "on team 338\n",
      "on team 2390\n",
      "on team 344\n",
      "on team 2393\n",
      "on team 349\n",
      "on team 2400\n",
      "on team 356\n",
      "on team 2916\n",
      "on team 2405\n",
      "on team 2415\n",
      "on team 2426\n",
      "on team 2428\n",
      "on team 2429\n",
      "on team 2433\n",
      "on team 2439\n",
      "on team 2440\n",
      "on team 2447\n",
      "on team 2448\n",
      "on team 2449\n",
      "on team 2450\n",
      "on team 399\n",
      "on team 2453\n",
      "on team 2458\n",
      "on team 2459\n",
      "on team 2460\n",
      "on team 2464\n",
      "on team 2466\n",
      "on team 2627\n",
      "on team 2483\n",
      "on team 2502\n",
      "on team 2504\n",
      "on team 2506\n",
      "on team 2509\n",
      "on team 2000\n",
      "on team 2005\n",
      "on team 2006\n",
      "on team 2010\n",
      "on team 2011\n",
      "on team 2523\n",
      "on team 2016\n",
      "on team 2529\n",
      "on team 2534\n",
      "on team 2535\n",
      "on team 2026\n",
      "on team 2029\n",
      "on team 2542\n",
      "on team 2032\n",
      "on team 2545\n",
      "on team 2546\n",
      "on team 2046\n"
     ]
    }
   ],
   "source": [
    "Xn = []\n",
    "yn = []\n",
    "for team in teams:\n",
    "    team1_df = teams_dict[team]\n",
    "    print(f\"on team {team}\")\n",
    "    \n",
    "    # loop over all possible games for a given team with enough prior games\n",
    "    for i in range(memory, len(team1_df)):\n",
    "        # find the two teams participating in the game\n",
    "        game_id = team1_df.loc[i][\"id\"]\n",
    "        both_teams_ids = games_df[games_df[\"id\"] == game_id][\"school_id\"]\n",
    "        both_teams_ids = list(both_teams_ids)\n",
    "\n",
    "        # throw out some odd cases where  there was only data for one of the teams in a given game\n",
    "        if len(both_teams_ids) != 2:\n",
    "            continue\n",
    "        #print(f\"both teams are {both_teams_ids}\")\n",
    " \n",
    "        # extract the dataframes for the two teams, find which game it was for them\n",
    "        t1_df = teams_dict[both_teams_ids[0]]#.loc[i-memory:i+1]\n",
    "        t2_df = teams_dict[both_teams_ids[1]]#.loc[i-memory:i+1]\n",
    "\n",
    "        t1_gamen = np.where(t1_df[\"id\"] == game_id)[0][0]\n",
    "        t2_gamen = np.where(t2_df[\"id\"] == game_id)[0][0]\n",
    "\n",
    "        # check to make sure that both teams have enough games in the past\n",
    "        if t1_gamen < memory or t2_gamen < memory:\n",
    "            continue\n",
    "        \n",
    "        # extract the games in recent history, dropping columns that don't go into the data\n",
    "        t1_df = t1_df.loc[t1_gamen-memory:t1_gamen]#+1]\n",
    "        t1_points = t1_df[\"points\"].to_numpy()[-1]\n",
    "        t2_df = t2_df.loc[t2_gamen-memory:t2_gamen]#+1]\n",
    "        t2_points = t2_df[\"points\"].to_numpy()[-1]\n",
    "        t1_df.drop(columns=[\"id\", \"school_id\", \"points\"], inplace=True)\n",
    "        t2_df.drop(columns=[\"id\", \"school_id\", \"points\"], inplace=True)\n",
    "\n",
    "        t1_array = np.array(t1_df)\n",
    "        t2_array = np.array(t2_df)\n",
    "\n",
    "        # break the arrays into past games and current game to go into training and test set\n",
    "        t1_past = t1_array[0:-1]\n",
    "        #t1_present = t1_array[-1]\n",
    "        t2_past = t2_array[0:-1]\n",
    "        #t2_present = t2_array[-1]\n",
    "\n",
    "\n",
    "        Xn.append(np.hstack([t1_past, t2_past]))\n",
    "        yn.append(np.array([t1_points, t2_points]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12742, 12, 56) (12742, 2)\n"
     ]
    }
   ],
   "source": [
    "Xn = np.array(Xn)\n",
    "yn = np.array(yn)\n",
    "print(Xn.shape, yn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xn_train, Xn_test, yn_train, yn_test = train_test_split(Xn, yn, test_size=0.25, random_state=42)\n",
    "\n",
    "n_train_samples = len(Xn_train)\n",
    "n_test_samples = len(Xn_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xn_train_flat = np.reshape(Xn_train, (n_train_samples, -1))\n",
    "Xn_test_flat = np.reshape(Xn_test, (n_test_samples, -1))\n",
    "scaler.fit(Xn_train_flat)\n",
    "Xn_train_scaled = scaler.transform(Xn_train_flat)\n",
    "Xn_test_scaled = scaler.transform(Xn_test_flat)\n",
    "Xn_train_scaled = np.reshape(Xn_train_scaled, (n_train_samples, memory, -1))\n",
    "Xn_test_scaled = np.reshape(Xn_test_scaled, (n_test_samples, memory, -1))\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "scaler2.fit(yn_train)\n",
    "yn_train_scaled = scaler2.transform(yn_train)\n",
    "yn_test_scaled = scaler2.transform(yn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 12, 56)            25312     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 12)                3312      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 26        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28650 (111.91 KB)\n",
      "Trainable params: 28650 (111.91 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units=Xn.shape[2], return_sequences=True, input_shape=(Xn_train_scaled.shape[1], Xn_train_scaled.shape[2])))\n",
    "model.add(LSTM(units=Xn.shape[1]))\n",
    "model.add(Dense(units=yn.shape[1]))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\natha\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\natha\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "299/299 [==============================] - 6s 8ms/step - loss: 0.7708\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 2s 7ms/step - loss: 0.7162\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 2s 7ms/step - loss: 0.6698\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.6158\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.5486\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.4804\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.4186\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.3612\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.3164\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.2757\n",
      "Epoch 11/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.2401\n",
      "Epoch 12/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.2130\n",
      "Epoch 13/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.1878\n",
      "Epoch 14/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.1653\n",
      "Epoch 15/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.1474\n",
      "Epoch 16/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.1318\n",
      "Epoch 17/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.1171\n",
      "Epoch 18/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.1053\n",
      "Epoch 19/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0945\n",
      "Epoch 20/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0861\n",
      "Epoch 21/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0780\n",
      "Epoch 22/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0725\n",
      "Epoch 23/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0650\n",
      "Epoch 24/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0601\n",
      "Epoch 25/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0570\n",
      "Epoch 26/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0511\n",
      "Epoch 27/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0471\n",
      "Epoch 28/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0460\n",
      "Epoch 29/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0417\n",
      "Epoch 30/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0382\n",
      "Epoch 31/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0380\n",
      "Epoch 32/100\n",
      "299/299 [==============================] - 3s 8ms/step - loss: 0.0364\n",
      "Epoch 33/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0334\n",
      "Epoch 34/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0328\n",
      "Epoch 35/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0316\n",
      "Epoch 36/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0304\n",
      "Epoch 37/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0271\n",
      "Epoch 38/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0271\n",
      "Epoch 39/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0265\n",
      "Epoch 40/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0269\n",
      "Epoch 41/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0249\n",
      "Epoch 42/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0226\n",
      "Epoch 43/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0222\n",
      "Epoch 44/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0227\n",
      "Epoch 45/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0222\n",
      "Epoch 46/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0201\n",
      "Epoch 47/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0201\n",
      "Epoch 48/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0211\n",
      "Epoch 49/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0208\n",
      "Epoch 50/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0204\n",
      "Epoch 51/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0169\n",
      "Epoch 52/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0168\n",
      "Epoch 53/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0174\n",
      "Epoch 54/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0185\n",
      "Epoch 55/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0173\n",
      "Epoch 56/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0165\n",
      "Epoch 57/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0163\n",
      "Epoch 58/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0167\n",
      "Epoch 59/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0162\n",
      "Epoch 60/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0166\n",
      "Epoch 61/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0151\n",
      "Epoch 62/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0145\n",
      "Epoch 63/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0155\n",
      "Epoch 64/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0147\n",
      "Epoch 65/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0143\n",
      "Epoch 66/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0137\n",
      "Epoch 67/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0138\n",
      "Epoch 68/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0139\n",
      "Epoch 69/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0144\n",
      "Epoch 70/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0153\n",
      "Epoch 71/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0145\n",
      "Epoch 72/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0133\n",
      "Epoch 73/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0127\n",
      "Epoch 74/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0122\n",
      "Epoch 75/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0125\n",
      "Epoch 76/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0125\n",
      "Epoch 77/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0129\n",
      "Epoch 78/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0125\n",
      "Epoch 79/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0129\n",
      "Epoch 80/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0122\n",
      "Epoch 81/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0118\n",
      "Epoch 82/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0117\n",
      "Epoch 83/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0120\n",
      "Epoch 84/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0123\n",
      "Epoch 85/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0113\n",
      "Epoch 86/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0113\n",
      "Epoch 87/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0116\n",
      "Epoch 88/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0118\n",
      "Epoch 89/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0112\n",
      "Epoch 90/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0108\n",
      "Epoch 91/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0115\n",
      "Epoch 92/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0113\n",
      "Epoch 93/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0105\n",
      "Epoch 94/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0105\n",
      "Epoch 95/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0112\n",
      "Epoch 96/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0109\n",
      "Epoch 97/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0105\n",
      "Epoch 98/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0095\n",
      "Epoch 99/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0097\n",
      "Epoch 100/100\n",
      "299/299 [==============================] - 2s 8ms/step - loss: 0.0103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e7234b8d10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xn_train_scaled, yn_train_scaled, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 12, 56)            25312     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 12, 56)            0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 12, 56)            25312     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 12, 56)            0         \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 56)                25312     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 56)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 114       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 76050 (297.07 KB)\n",
      "Trainable params: 76050 (297.07 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "model3 = Sequential()\n",
    "\n",
    "model3.add(LSTM(units=Xn.shape[2], return_sequences=True, input_shape=(Xn_train_scaled.shape[1], Xn_train_scaled.shape[2])))\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "model3.add(LSTM(units=Xn.shape[2], return_sequences=True))\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "model3.add(LSTM(units=Xn.shape[2]))\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "model3.add(Dense(units=yn.shape[1]))\n",
    "\n",
    "model3.summary()\n",
    "model3.compile(loss=\"mean_squared_error\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "299/299 [==============================] - 9s 12ms/step - loss: 0.7969\n",
      "Epoch 2/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.7378\n",
      "Epoch 3/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.7009\n",
      "Epoch 4/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.6445\n",
      "Epoch 5/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.5838\n",
      "Epoch 6/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.5176\n",
      "Epoch 7/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.4519\n",
      "Epoch 8/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.4031\n",
      "Epoch 9/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.3538\n",
      "Epoch 10/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.3139\n",
      "Epoch 11/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.2791\n",
      "Epoch 12/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.2493\n",
      "Epoch 13/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.2265\n",
      "Epoch 14/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.2034\n",
      "Epoch 15/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.1844\n",
      "Epoch 16/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.1690\n",
      "Epoch 17/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.1569\n",
      "Epoch 18/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.1462\n",
      "Epoch 19/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.1364\n",
      "Epoch 20/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.1299\n",
      "Epoch 21/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.1241\n",
      "Epoch 22/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.1165\n",
      "Epoch 23/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.1088\n",
      "Epoch 24/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.1025\n",
      "Epoch 25/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.1004\n",
      "Epoch 26/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0959\n",
      "Epoch 27/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0913\n",
      "Epoch 28/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0891\n",
      "Epoch 29/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0859\n",
      "Epoch 30/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0818\n",
      "Epoch 31/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0808\n",
      "Epoch 32/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0787\n",
      "Epoch 33/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0756\n",
      "Epoch 34/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0744\n",
      "Epoch 35/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0721\n",
      "Epoch 36/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0718\n",
      "Epoch 37/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0686\n",
      "Epoch 38/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0671\n",
      "Epoch 39/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0649\n",
      "Epoch 40/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0653\n",
      "Epoch 41/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0633\n",
      "Epoch 42/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0629\n",
      "Epoch 43/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0618\n",
      "Epoch 44/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0614\n",
      "Epoch 45/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0590\n",
      "Epoch 46/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0588\n",
      "Epoch 47/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0579\n",
      "Epoch 48/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0572\n",
      "Epoch 49/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0559\n",
      "Epoch 50/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0552\n",
      "Epoch 51/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0560\n",
      "Epoch 52/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0541\n",
      "Epoch 53/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0527\n",
      "Epoch 54/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0524\n",
      "Epoch 55/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0508\n",
      "Epoch 56/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0518\n",
      "Epoch 57/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0513\n",
      "Epoch 58/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0487\n",
      "Epoch 59/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0487\n",
      "Epoch 60/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0494\n",
      "Epoch 61/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0485\n",
      "Epoch 62/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0483\n",
      "Epoch 63/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0477\n",
      "Epoch 64/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0493\n",
      "Epoch 65/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0471\n",
      "Epoch 66/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0466\n",
      "Epoch 67/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0452\n",
      "Epoch 68/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0456\n",
      "Epoch 69/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0454\n",
      "Epoch 70/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0444\n",
      "Epoch 71/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0455\n",
      "Epoch 72/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0443\n",
      "Epoch 73/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0438\n",
      "Epoch 74/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0434\n",
      "Epoch 75/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0428\n",
      "Epoch 76/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0428\n",
      "Epoch 77/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0424\n",
      "Epoch 78/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0428\n",
      "Epoch 79/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0431\n",
      "Epoch 80/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0416\n",
      "Epoch 81/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0404\n",
      "Epoch 82/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0407\n",
      "Epoch 83/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0409\n",
      "Epoch 84/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0405\n",
      "Epoch 85/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0412\n",
      "Epoch 86/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0407\n",
      "Epoch 87/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0407\n",
      "Epoch 88/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0397\n",
      "Epoch 89/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0401\n",
      "Epoch 90/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0385\n",
      "Epoch 91/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0386\n",
      "Epoch 92/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0392\n",
      "Epoch 93/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0392\n",
      "Epoch 94/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0384\n",
      "Epoch 95/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0376\n",
      "Epoch 96/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0373\n",
      "Epoch 97/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0379\n",
      "Epoch 98/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0367\n",
      "Epoch 99/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0371\n",
      "Epoch 100/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0366\n",
      "Epoch 101/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0366\n",
      "Epoch 102/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0372\n",
      "Epoch 103/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0369\n",
      "Epoch 104/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0360\n",
      "Epoch 105/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0365\n",
      "Epoch 106/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0360\n",
      "Epoch 107/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0367\n",
      "Epoch 108/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0358\n",
      "Epoch 109/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0358\n",
      "Epoch 110/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0349\n",
      "Epoch 111/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0340\n",
      "Epoch 112/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0352\n",
      "Epoch 113/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0347\n",
      "Epoch 114/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0346\n",
      "Epoch 115/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0341\n",
      "Epoch 116/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0346\n",
      "Epoch 117/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0335\n",
      "Epoch 118/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0337\n",
      "Epoch 119/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0338\n",
      "Epoch 120/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0338\n",
      "Epoch 121/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0329\n",
      "Epoch 122/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0337\n",
      "Epoch 123/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0340\n",
      "Epoch 124/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0338\n",
      "Epoch 125/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0332\n",
      "Epoch 126/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0331\n",
      "Epoch 127/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0331\n",
      "Epoch 128/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0321\n",
      "Epoch 129/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0320\n",
      "Epoch 130/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0322\n",
      "Epoch 131/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0322\n",
      "Epoch 132/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0322\n",
      "Epoch 133/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0324\n",
      "Epoch 134/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0317\n",
      "Epoch 135/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0315\n",
      "Epoch 136/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0311\n",
      "Epoch 137/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0313\n",
      "Epoch 138/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0316\n",
      "Epoch 139/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0316\n",
      "Epoch 140/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0316\n",
      "Epoch 141/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0305\n",
      "Epoch 142/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0308\n",
      "Epoch 143/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0304\n",
      "Epoch 144/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0304\n",
      "Epoch 145/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0314\n",
      "Epoch 146/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0313\n",
      "Epoch 147/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0305\n",
      "Epoch 148/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0304\n",
      "Epoch 149/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0298\n",
      "Epoch 150/200\n",
      "299/299 [==============================] - 4s 12ms/step - loss: 0.0306\n",
      "Epoch 151/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0304\n",
      "Epoch 152/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0294\n",
      "Epoch 153/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0295\n",
      "Epoch 154/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0299\n",
      "Epoch 155/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0300\n",
      "Epoch 156/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0298\n",
      "Epoch 157/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0304\n",
      "Epoch 158/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0291\n",
      "Epoch 159/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0297\n",
      "Epoch 160/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0293\n",
      "Epoch 161/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0297\n",
      "Epoch 162/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0292\n",
      "Epoch 163/200\n",
      "299/299 [==============================] - 4s 14ms/step - loss: 0.0294\n",
      "Epoch 164/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0289\n",
      "Epoch 165/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0293\n",
      "Epoch 166/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0292\n",
      "Epoch 167/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0290\n",
      "Epoch 168/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0287\n",
      "Epoch 169/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0286\n",
      "Epoch 170/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0288\n",
      "Epoch 171/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0283\n",
      "Epoch 172/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0286\n",
      "Epoch 173/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0284\n",
      "Epoch 174/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0279\n",
      "Epoch 175/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0283\n",
      "Epoch 176/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0287\n",
      "Epoch 177/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0281\n",
      "Epoch 178/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0282\n",
      "Epoch 179/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0278\n",
      "Epoch 180/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0278\n",
      "Epoch 181/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0282\n",
      "Epoch 182/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0277\n",
      "Epoch 183/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0282\n",
      "Epoch 184/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0278\n",
      "Epoch 185/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0273\n",
      "Epoch 186/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0269\n",
      "Epoch 187/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0272\n",
      "Epoch 188/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0277\n",
      "Epoch 189/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0275\n",
      "Epoch 190/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0275\n",
      "Epoch 191/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0276\n",
      "Epoch 192/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0265\n",
      "Epoch 193/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0266\n",
      "Epoch 194/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0272\n",
      "Epoch 195/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0269\n",
      "Epoch 196/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0265\n",
      "Epoch 197/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0264\n",
      "Epoch 198/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0272\n",
      "Epoch 199/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0270\n",
      "Epoch 200/200\n",
      "299/299 [==============================] - 4s 13ms/step - loss: 0.0271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e73a9f7150>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(Xn_train_scaled, yn_train_scaled, epochs=200, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 7ms/step\n",
      "0.31027311508729793\n"
     ]
    }
   ],
   "source": [
    "yn_pred = model3.predict(Xn_test_scaled)\n",
    "print(mean_squared_error(yn_test_scaled, yn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 5ms/step\n",
      "62.04484006732192\n",
      "3.9937795180449123\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "yn_pred = model3.predict(Xn_test_scaled)\n",
    "yn_pred = scaler2.inverse_transform(yn_pred)\n",
    "print(mean_squared_error(yn_test, yn_pred))\n",
    "print(mean_absolute_error(yn_test, yn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.031242  9.477807]\n",
      " [33.32205  46.80371 ]\n",
      " [36.354    24.018084]\n",
      " ...\n",
      " [34.021908 12.794992]\n",
      " [ 9.149614 27.79305 ]\n",
      " [12.484794  4.638334]]\n",
      "[[13  9]\n",
      " [31 48]\n",
      " [37 24]\n",
      " ...\n",
      " [35 13]\n",
      " [20 37]\n",
      " [14  6]]\n"
     ]
    }
   ],
   "source": [
    "print(yn_pred)\n",
    "print(yn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct games is: 2886 out of 3186, pct correct is 0.9058380414312618.\n"
     ]
    }
   ],
   "source": [
    "real_games = []\n",
    "pred_games = []\n",
    "for i in range(len(yn_test)):\n",
    "    if yn_test[i][0] > yn_test[i][1]:\n",
    "        real_games.append(1)\n",
    "    else:\n",
    "        real_games.append(0)\n",
    "    if yn_pred[i][0] > yn_pred[i][1]:\n",
    "        pred_games.append(1)\n",
    "    else:\n",
    "        pred_games.append(0)\n",
    "\n",
    "incorrect_games = np.sum(np.abs(np.array(pred_games) - np.array(real_games)))\n",
    "correct_games = len(yn_test) - incorrect_games\n",
    "print(f\"correct games is: {correct_games} out of {len(yn_test)}, pct correct is {(correct_games)/(correct_games + incorrect_games)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12742, 12, 56) (12742, 56)\n"
     ]
    }
   ],
   "source": [
    "Xn = np.array(Xn)\n",
    "yn = np.array(yn)\n",
    "print(Xn.shape, yn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xn_train, Xn_test, yn_train, yn_test = train_test_split(Xn, yn, test_size=0.25, random_state=42)\n",
    "\n",
    "n_train_samples = len(Xn_train)\n",
    "n_test_samples = len(Xn_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xn_train_flat = np.reshape(Xn_train, (n_train_samples, -1))\n",
    "Xn_test_flat = np.reshape(Xn_test, (n_test_samples, -1))\n",
    "scaler.fit(Xn_train_flat)\n",
    "Xn_train_scaled = scaler.transform(Xn_train_flat)\n",
    "Xn_test_scaled = scaler.transform(Xn_test_flat)\n",
    "Xn_train_scaled = np.reshape(Xn_train_scaled, (n_train_samples, memory, -1))\n",
    "Xn_test_scaled = np.reshape(Xn_test_scaled, (n_test_samples, memory, -1))\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "scaler2.fit(yn_train)\n",
    "yn_train_scaled = scaler2.transform(yn_train)\n",
    "yn_test_scaled = scaler2.transform(yn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units=yn.shape[1], input_shape=(Xn_train_scaled.shape[1], Xn_train_scaled.shape[2])))\n",
    "model.add(Dense(units=yn.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_1 (LSTM)               (None, 56)                25312     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 56)                3192      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28504 (111.34 KB)\n",
      "Trainable params: 28504 (111.34 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\natha\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\natha\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "299/299 [==============================] - 4s 5ms/step - loss: 0.8993\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.8450\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.8290\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.8150\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.8006\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.7856\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.7691\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.7520\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.7340\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.7166\n",
      "Epoch 11/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.6995\n",
      "Epoch 12/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.6839\n",
      "Epoch 13/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.6694\n",
      "Epoch 14/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.6559\n",
      "Epoch 15/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.6435\n",
      "Epoch 16/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.6321\n",
      "Epoch 17/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.6212\n",
      "Epoch 18/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.6115\n",
      "Epoch 19/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.6027\n",
      "Epoch 20/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.5939\n",
      "Epoch 21/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.5856\n",
      "Epoch 22/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.5779\n",
      "Epoch 23/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.5709\n",
      "Epoch 24/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.5642\n",
      "Epoch 25/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.5577\n",
      "Epoch 26/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.5515\n",
      "Epoch 27/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.5454\n",
      "Epoch 28/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.5404\n",
      "Epoch 29/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.5355\n",
      "Epoch 30/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.5302\n",
      "Epoch 31/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.5250\n",
      "Epoch 32/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.5210\n",
      "Epoch 33/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.5169\n",
      "Epoch 34/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.5125\n",
      "Epoch 35/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.5087\n",
      "Epoch 36/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.5045\n",
      "Epoch 37/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.5010\n",
      "Epoch 38/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4971\n",
      "Epoch 39/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4941\n",
      "Epoch 40/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4917\n",
      "Epoch 41/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4879\n",
      "Epoch 42/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4846\n",
      "Epoch 43/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4827\n",
      "Epoch 44/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4791\n",
      "Epoch 45/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4771\n",
      "Epoch 46/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4735\n",
      "Epoch 47/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4719\n",
      "Epoch 48/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4692\n",
      "Epoch 49/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4673\n",
      "Epoch 50/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4649\n",
      "Epoch 51/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4631\n",
      "Epoch 52/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4605\n",
      "Epoch 53/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4588\n",
      "Epoch 54/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4568\n",
      "Epoch 55/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4547\n",
      "Epoch 56/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4533\n",
      "Epoch 57/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4507\n",
      "Epoch 58/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4492\n",
      "Epoch 59/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4488\n",
      "Epoch 60/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4458\n",
      "Epoch 61/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4446\n",
      "Epoch 62/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4429\n",
      "Epoch 63/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4418\n",
      "Epoch 64/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4400\n",
      "Epoch 65/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4382\n",
      "Epoch 66/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4369\n",
      "Epoch 67/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4357\n",
      "Epoch 68/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4350\n",
      "Epoch 69/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4327\n",
      "Epoch 70/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4316\n",
      "Epoch 71/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4295\n",
      "Epoch 72/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4293\n",
      "Epoch 73/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4293\n",
      "Epoch 74/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4275\n",
      "Epoch 75/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4259\n",
      "Epoch 76/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4245\n",
      "Epoch 77/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4237\n",
      "Epoch 78/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4232\n",
      "Epoch 79/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4229\n",
      "Epoch 80/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4195\n",
      "Epoch 81/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4197\n",
      "Epoch 82/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4181\n",
      "Epoch 83/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4173\n",
      "Epoch 84/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4180\n",
      "Epoch 85/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4157\n",
      "Epoch 86/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4148\n",
      "Epoch 87/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4137\n",
      "Epoch 88/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4125\n",
      "Epoch 89/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4123\n",
      "Epoch 90/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4124\n",
      "Epoch 91/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4111\n",
      "Epoch 92/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4110\n",
      "Epoch 93/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4082\n",
      "Epoch 94/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4075\n",
      "Epoch 95/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4082\n",
      "Epoch 96/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4077\n",
      "Epoch 97/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4061\n",
      "Epoch 98/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4065\n",
      "Epoch 99/100\n",
      "299/299 [==============================] - 2s 5ms/step - loss: 0.4044\n",
      "Epoch 100/100\n",
      "299/299 [==============================] - 1s 5ms/step - loss: 0.4039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x213ba2612d0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xn_train_scaled, yn_train_scaled, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "yn_pred = model.predict(Xn_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7059745207355442\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(yn_test_scaled, yn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = scaler2.inverse_transform(yn_test_scaled) - scaler2.inverse_transform(yn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs_df = pd.DataFrame(diffs[:,0:28], columns=t1_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t1_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fumblesRecovered</th>\n",
       "      <th>rushingTDs</th>\n",
       "      <th>passingTDs</th>\n",
       "      <th>firstDowns</th>\n",
       "      <th>thirdDownEff</th>\n",
       "      <th>totalYards</th>\n",
       "      <th>netPassingYards</th>\n",
       "      <th>yardsPerPass</th>\n",
       "      <th>rushingYards</th>\n",
       "      <th>rushingAttempts</th>\n",
       "      <th>...</th>\n",
       "      <th>opposing_yardsPerRushAttempt</th>\n",
       "      <th>opposing_passingTDs</th>\n",
       "      <th>opposing_rushingTDs</th>\n",
       "      <th>thirdDownConverts</th>\n",
       "      <th>thirdDownAttempts</th>\n",
       "      <th>fourthDownConverts</th>\n",
       "      <th>fourthDownAttempts</th>\n",
       "      <th>completions</th>\n",
       "      <th>passAttempts</th>\n",
       "      <th>passCompletionPercentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.787471</td>\n",
       "      <td>-0.623071</td>\n",
       "      <td>-1.280493</td>\n",
       "      <td>-2.716593</td>\n",
       "      <td>0.160488</td>\n",
       "      <td>-61.874268</td>\n",
       "      <td>-20.760956</td>\n",
       "      <td>-0.059482</td>\n",
       "      <td>-41.374687</td>\n",
       "      <td>2.470778</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.832166</td>\n",
       "      <td>-1.716744</td>\n",
       "      <td>-2.261754</td>\n",
       "      <td>2.454082</td>\n",
       "      <td>0.619487</td>\n",
       "      <td>-1.669718</td>\n",
       "      <td>-1.669718</td>\n",
       "      <td>-2.460936</td>\n",
       "      <td>-2.634693</td>\n",
       "      <td>-0.038308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.135716</td>\n",
       "      <td>2.649791</td>\n",
       "      <td>-1.220105</td>\n",
       "      <td>2.644871</td>\n",
       "      <td>0.058802</td>\n",
       "      <td>17.996979</td>\n",
       "      <td>12.228912</td>\n",
       "      <td>0.728169</td>\n",
       "      <td>5.655609</td>\n",
       "      <td>0.155582</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.429580</td>\n",
       "      <td>-2.030435</td>\n",
       "      <td>0.754238</td>\n",
       "      <td>0.425841</td>\n",
       "      <td>-0.895601</td>\n",
       "      <td>-1.386396</td>\n",
       "      <td>-1.386395</td>\n",
       "      <td>2.097589</td>\n",
       "      <td>-4.476471</td>\n",
       "      <td>0.141760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.221952</td>\n",
       "      <td>0.919461</td>\n",
       "      <td>-0.049857</td>\n",
       "      <td>1.063900</td>\n",
       "      <td>0.137590</td>\n",
       "      <td>4.400085</td>\n",
       "      <td>30.666962</td>\n",
       "      <td>3.139234</td>\n",
       "      <td>-26.323242</td>\n",
       "      <td>-6.516758</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.916096</td>\n",
       "      <td>0.437135</td>\n",
       "      <td>-0.168412</td>\n",
       "      <td>-2.265317</td>\n",
       "      <td>-7.199229</td>\n",
       "      <td>0.007008</td>\n",
       "      <td>0.007009</td>\n",
       "      <td>-0.148223</td>\n",
       "      <td>-5.221842</td>\n",
       "      <td>0.121046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.101742</td>\n",
       "      <td>2.602601</td>\n",
       "      <td>0.360394</td>\n",
       "      <td>2.525581</td>\n",
       "      <td>0.006127</td>\n",
       "      <td>66.552429</td>\n",
       "      <td>42.580994</td>\n",
       "      <td>-0.299557</td>\n",
       "      <td>24.054413</td>\n",
       "      <td>-2.735428</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167954</td>\n",
       "      <td>-0.303723</td>\n",
       "      <td>1.000788</td>\n",
       "      <td>-0.222930</td>\n",
       "      <td>-0.223150</td>\n",
       "      <td>-0.205026</td>\n",
       "      <td>-0.205026</td>\n",
       "      <td>4.234775</td>\n",
       "      <td>7.268991</td>\n",
       "      <td>0.045232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.509203</td>\n",
       "      <td>0.247883</td>\n",
       "      <td>-0.566114</td>\n",
       "      <td>-0.691948</td>\n",
       "      <td>0.003632</td>\n",
       "      <td>-1.638763</td>\n",
       "      <td>21.916595</td>\n",
       "      <td>1.549952</td>\n",
       "      <td>-23.377289</td>\n",
       "      <td>-8.506020</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246606</td>\n",
       "      <td>-0.216064</td>\n",
       "      <td>-2.185653</td>\n",
       "      <td>-0.780818</td>\n",
       "      <td>-2.056944</td>\n",
       "      <td>-2.504359</td>\n",
       "      <td>-2.504359</td>\n",
       "      <td>-2.251232</td>\n",
       "      <td>-2.164650</td>\n",
       "      <td>-0.024807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3181</th>\n",
       "      <td>-0.732223</td>\n",
       "      <td>0.221433</td>\n",
       "      <td>-0.821900</td>\n",
       "      <td>-1.479908</td>\n",
       "      <td>-0.097866</td>\n",
       "      <td>-6.562836</td>\n",
       "      <td>-6.710770</td>\n",
       "      <td>-0.000724</td>\n",
       "      <td>0.279190</td>\n",
       "      <td>3.676392</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.217415</td>\n",
       "      <td>-0.866635</td>\n",
       "      <td>-0.786627</td>\n",
       "      <td>-1.600833</td>\n",
       "      <td>-1.414748</td>\n",
       "      <td>-0.764393</td>\n",
       "      <td>-0.764393</td>\n",
       "      <td>1.462877</td>\n",
       "      <td>-0.792961</td>\n",
       "      <td>0.063101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3182</th>\n",
       "      <td>0.442357</td>\n",
       "      <td>1.207644</td>\n",
       "      <td>-0.370953</td>\n",
       "      <td>1.774637</td>\n",
       "      <td>0.145565</td>\n",
       "      <td>-14.824585</td>\n",
       "      <td>-1.131027</td>\n",
       "      <td>-1.302375</td>\n",
       "      <td>-13.914894</td>\n",
       "      <td>6.366629</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355629</td>\n",
       "      <td>-1.019268</td>\n",
       "      <td>0.077661</td>\n",
       "      <td>4.356813</td>\n",
       "      <td>5.663538</td>\n",
       "      <td>1.373217</td>\n",
       "      <td>1.373217</td>\n",
       "      <td>3.492224</td>\n",
       "      <td>4.312843</td>\n",
       "      <td>0.031985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3183</th>\n",
       "      <td>0.074386</td>\n",
       "      <td>-1.306584</td>\n",
       "      <td>0.075787</td>\n",
       "      <td>-2.692142</td>\n",
       "      <td>-0.101866</td>\n",
       "      <td>-47.222565</td>\n",
       "      <td>54.809769</td>\n",
       "      <td>0.222544</td>\n",
       "      <td>-101.785736</td>\n",
       "      <td>-6.917194</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.965228</td>\n",
       "      <td>-0.251059</td>\n",
       "      <td>0.099351</td>\n",
       "      <td>-0.951893</td>\n",
       "      <td>0.598404</td>\n",
       "      <td>-0.669117</td>\n",
       "      <td>-0.669117</td>\n",
       "      <td>0.759758</td>\n",
       "      <td>4.773159</td>\n",
       "      <td>-0.079777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3184</th>\n",
       "      <td>0.316352</td>\n",
       "      <td>-0.991813</td>\n",
       "      <td>1.125704</td>\n",
       "      <td>0.162011</td>\n",
       "      <td>0.094831</td>\n",
       "      <td>66.242218</td>\n",
       "      <td>118.429260</td>\n",
       "      <td>2.101486</td>\n",
       "      <td>-52.163979</td>\n",
       "      <td>-11.531380</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.299980</td>\n",
       "      <td>-0.396057</td>\n",
       "      <td>0.554286</td>\n",
       "      <td>1.803398</td>\n",
       "      <td>1.623865</td>\n",
       "      <td>1.068068</td>\n",
       "      <td>1.068068</td>\n",
       "      <td>4.942402</td>\n",
       "      <td>9.621449</td>\n",
       "      <td>0.015711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3185</th>\n",
       "      <td>-1.112951</td>\n",
       "      <td>-0.171755</td>\n",
       "      <td>-0.239393</td>\n",
       "      <td>-3.694746</td>\n",
       "      <td>-0.020355</td>\n",
       "      <td>-94.913788</td>\n",
       "      <td>-75.901428</td>\n",
       "      <td>-2.348426</td>\n",
       "      <td>-19.004562</td>\n",
       "      <td>-7.221436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.929730</td>\n",
       "      <td>-0.262628</td>\n",
       "      <td>-0.134632</td>\n",
       "      <td>-0.958741</td>\n",
       "      <td>-2.151088</td>\n",
       "      <td>-1.892063</td>\n",
       "      <td>-1.892063</td>\n",
       "      <td>-1.975716</td>\n",
       "      <td>-1.084908</td>\n",
       "      <td>-0.034078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3186 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fumblesRecovered  rushingTDs  passingTDs  firstDowns  thirdDownEff  \\\n",
       "0            -0.787471   -0.623071   -1.280493   -2.716593      0.160488   \n",
       "1             0.135716    2.649791   -1.220105    2.644871      0.058802   \n",
       "2            -1.221952    0.919461   -0.049857    1.063900      0.137590   \n",
       "3             0.101742    2.602601    0.360394    2.525581      0.006127   \n",
       "4            -1.509203    0.247883   -0.566114   -0.691948      0.003632   \n",
       "...                ...         ...         ...         ...           ...   \n",
       "3181         -0.732223    0.221433   -0.821900   -1.479908     -0.097866   \n",
       "3182          0.442357    1.207644   -0.370953    1.774637      0.145565   \n",
       "3183          0.074386   -1.306584    0.075787   -2.692142     -0.101866   \n",
       "3184          0.316352   -0.991813    1.125704    0.162011      0.094831   \n",
       "3185         -1.112951   -0.171755   -0.239393   -3.694746     -0.020355   \n",
       "\n",
       "      totalYards  netPassingYards  yardsPerPass  rushingYards  \\\n",
       "0     -61.874268       -20.760956     -0.059482    -41.374687   \n",
       "1      17.996979        12.228912      0.728169      5.655609   \n",
       "2       4.400085        30.666962      3.139234    -26.323242   \n",
       "3      66.552429        42.580994     -0.299557     24.054413   \n",
       "4      -1.638763        21.916595      1.549952    -23.377289   \n",
       "...          ...              ...           ...           ...   \n",
       "3181   -6.562836        -6.710770     -0.000724      0.279190   \n",
       "3182  -14.824585        -1.131027     -1.302375    -13.914894   \n",
       "3183  -47.222565        54.809769      0.222544   -101.785736   \n",
       "3184   66.242218       118.429260      2.101486    -52.163979   \n",
       "3185  -94.913788       -75.901428     -2.348426    -19.004562   \n",
       "\n",
       "      rushingAttempts  ...  opposing_yardsPerRushAttempt  opposing_passingTDs  \\\n",
       "0            2.470778  ...                     -0.832166            -1.716744   \n",
       "1            0.155582  ...                     -1.429580            -2.030435   \n",
       "2           -6.516758  ...                     -0.916096             0.437135   \n",
       "3           -2.735428  ...                     -0.167954            -0.303723   \n",
       "4           -8.506020  ...                     -0.246606            -0.216064   \n",
       "...               ...  ...                           ...                  ...   \n",
       "3181         3.676392  ...                     -1.217415            -0.866635   \n",
       "3182         6.366629  ...                      0.355629            -1.019268   \n",
       "3183        -6.917194  ...                     -0.965228            -0.251059   \n",
       "3184       -11.531380  ...                     -0.299980            -0.396057   \n",
       "3185        -7.221436  ...                     -0.929730            -0.262628   \n",
       "\n",
       "      opposing_rushingTDs  thirdDownConverts  thirdDownAttempts  \\\n",
       "0               -2.261754           2.454082           0.619487   \n",
       "1                0.754238           0.425841          -0.895601   \n",
       "2               -0.168412          -2.265317          -7.199229   \n",
       "3                1.000788          -0.222930          -0.223150   \n",
       "4               -2.185653          -0.780818          -2.056944   \n",
       "...                   ...                ...                ...   \n",
       "3181            -0.786627          -1.600833          -1.414748   \n",
       "3182             0.077661           4.356813           5.663538   \n",
       "3183             0.099351          -0.951893           0.598404   \n",
       "3184             0.554286           1.803398           1.623865   \n",
       "3185            -0.134632          -0.958741          -2.151088   \n",
       "\n",
       "      fourthDownConverts  fourthDownAttempts  completions  passAttempts  \\\n",
       "0              -1.669718           -1.669718    -2.460936     -2.634693   \n",
       "1              -1.386396           -1.386395     2.097589     -4.476471   \n",
       "2               0.007008            0.007009    -0.148223     -5.221842   \n",
       "3              -0.205026           -0.205026     4.234775      7.268991   \n",
       "4              -2.504359           -2.504359    -2.251232     -2.164650   \n",
       "...                  ...                 ...          ...           ...   \n",
       "3181           -0.764393           -0.764393     1.462877     -0.792961   \n",
       "3182            1.373217            1.373217     3.492224      4.312843   \n",
       "3183           -0.669117           -0.669117     0.759758      4.773159   \n",
       "3184            1.068068            1.068068     4.942402      9.621449   \n",
       "3185           -1.892063           -1.892063    -1.975716     -1.084908   \n",
       "\n",
       "      passCompletionPercentage  \n",
       "0                    -0.038308  \n",
       "1                     0.141760  \n",
       "2                     0.121046  \n",
       "3                     0.045232  \n",
       "4                    -0.024807  \n",
       "...                        ...  \n",
       "3181                  0.063101  \n",
       "3182                  0.031985  \n",
       "3183                 -0.079777  \n",
       "3184                  0.015711  \n",
       "3185                 -0.034078  \n",
       "\n",
       "[3186 rows x 28 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
